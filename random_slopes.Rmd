```{r chunk_setup-ran-slope, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(
  # cache
  cache.rebuild = F,
  cache         = T
)
```

# 추가 Random Effect

앞에서 랜덤 절편을 살펴보았지만, 클러스터마다 어떤 관측값이나 낮은 레벨의 covariate 효과도 변하게 할 수 있다.
이는 효과가 다른 관측값의 수준/값에 따라 변하도록 하는 *interaction(교호작용)* 개념과 동일하다.

## 적용

GPA 데이터에 돌아가서, 앞선 시각화를 상기해 보자. 
학생들 사이의 차이를 강조하기 위해 애니메이션으로 표현해 보았다.
시간에 따라 일반적으로 증가하지만, 어떤 학생은 상대적으로 평평하거나 일직선이 아니다. 
이러한 현상을 어떻게 포착할 수 있을까?

```{r spaghetti2-old, echo=FALSE, eval=FALSE}
set.seed(1234)
gpa_lm = lm(gpa ~ occasion, data = gpa)

init = gpa %>%
  modelr::add_predictions(gpa_lm, var = 'all') %>%
  mutate(select = factor(student %in% sample(1:200, 10)),
         sz = c(.5, 1)[select]) %>%
  group_by(student, select) 

init %>%
  plot_ly %>%
  add_lines(
    x =  ~ occasion,
    y =  ~ gpa,
    size = I(.5),
    opacity = .35,
    color =  ~ select,
    size = ~ sz,
    colors = scico::scico(2, begin = .25),
    showlegend = F
  ) %>%
  add_lines(
    x =  ~ occasion,
    y =  ~ gpa,
    opacity = .35,
    color =  ~ select,
    size = I(2),
    colors = scico::scico(2, begin = .25),
    data = filter(init, select == TRUE),
    showlegend = F
  ) %>%
  add_lines(
    x =  ~ occasion,
    y =  ~ all,
    color = I(palettes$stan_red$stan_red),
    opacity = .70
  ) %>%
  theme_plotly()
```

```{r spaghetti2-anim, echo=FALSE, eval=FALSE}
options(gganimate.dev_args = list(bg = '#fffff8'))
gpa %>%
  slice(1:120) %>%   # if you don't want to wait
  ggplot(aes(x = occasion, y = gpa)) +
  geom_path(aes(group = student, alpha = occasion),
            color = '#ff5500',
            arrow = arrow(type = 'closed',
                          angle = 45,
                          length = unit(2, 'points')),
            show.legend = F) +
  # geom_smooth(
  #   aes(alpha = occasion),
  #   se = F,
  #   color = 'darkred',
  #   show.legend = F
  # ) +
  transition_time(as.integer(student)) +
  shadow_mark(colour = 'gray50', size = 0.25) +
  theme_clean()

anim_save('img/gpa_anim.gif')
```

```{r spaghetti2-anim-show, echo=F, out.width='50%'}
knitr::include_graphics('img/gpa_anim.gif')
```


<br>
이 문제를 해결하기 위해, 절편뿐만 아니라 시간에 따른 변화도 학생에 따라 변할 수 있다고 가정해 보자.
<span class="pack">lme4</span> 를 사용하면 바로할 수 있다.

```{r random_slope, eval=FALSE}
gpa_mixed =  lmer(gpa ~ occasion + (1 + occasion | student), data = gpa)
summary(gpa_mixed)
```

꽤 쉽지 않은가? 괄호 안에서, 바 `|` 왼편에 대부분 모델링 함수[^nointneeded]에서 하는 것처럼 모델 공식을 위치시키는 것이다. 결과를 보자.


```{r random_slope_summary, echo=FALSE}
gpa_mixed =  lmer(gpa ~ occasion + (1 + occasion | student), data = gpa)

extract_fixed_effects(gpa_mixed) %>% 
  select(-(t:p_value)) %>% 
  kable_df()

extract_vc(gpa_mixed, ci_level = 0) %>% 
  kable_df()

# gpa_mixed %>%
#   tidy('fixed', conf.int=T) %>%
#   kable_df()
# 
# 
# data.frame(VarCorr(gpa_mixed)) %>% 
#   slice(-3) %>%  
#   select(-var2) %>% 
#   rename(variance=vcov, sd=sdcor, re=var1) %>%  
#   mutate_all(function(x) ifelse(is.na(x), '', x)) %>%
#   data.frame %>%
#   kable_df()
```

이전에서처럼, 0 을 첫 학기로 하고 있으므로, 절편은 첫 학기의 평균 학점을 의미하게 된다.
이전과 같이 학기 계수는 한 학기의 학점변화량을 의미한다.
모형의 fixed effect 부분에 변화가 없으므로, 값은 전과 같게 된다. 

절편 분산은 시작학기의 평점이 학생마다 얼마나 차이가 나는지를 의미한다. 
학기 효과의 분산은 비교할 것이 없는 것 같지만, 기울기는 절편보다 더 스케일에 큰 차이가 있다. 
학기에 대한 평균 기울기, 즉 fixed 효과는 `r round(fixef(gpa_mixed)[2], 2)` 이지만, 학생마다 절반 정도 차이가 난다. 
따라서 대부분의 학생들이 0 을 가지는 flat effect 에서 population 평균의 두배 이상의 값 사이의 한 값을 가지리라 예상할 수 있다[^sdslopes]. 

절편과 기울기의 correlation 은 다른 흥미로운 포인트이다. 
우리 경우는 `r data.frame(VarCorr(gpa_mixed)) %>% slice(3) %>% select(sdcor) %>%  round(2)` 이다. 
꽤 작은 편이지만 다른 correlation 과 해석은 비슷하다. 
이 경우 절편값이 작을수록 기울기가 커지게 된다. 
일반적으로 사람들 성적이 좋아지고 잘 못하는 사람은 개선될 여지가 더 많을 것이라는 것은 직관적이다. 
하지만, 매우 약하고, 실제로는 여기에 큰 비중을 둘 필요가 없다.


## 개별 회귀모델과 비교

이러한 결과를 각 학생에 대해 분리된 회귀를 실행했을 때의 결과와 비교해보자. 다음에서 모든 학생들에 대한 절편과 기울기 계수의 분포를 볼 수 있다. occansion x student interaction[^fe_comparison]이 있는 fixed 효과모델에서 추정한 것과 같다는 것을 주목하라.

```{r ranints_vs_separateints, echo=FALSE, eval=FALSE}
gint = tibble(
  Mixed = extract_random_coefs(gpa_mixed) %>% filter(effect == 'Intercept') %>% pull(value),
  Separate = gpa_lm_by_group %>% pull(Intercept)
) %>%
  pivot_longer(everything(),  names_to= 'Model', values_to = 'Intercept') %>%
  ggplot(aes(x = Intercept)) +
  geom_density(aes(color = Model, fill = Model), alpha = .25) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(x = '', y = '', title = 'Intercepts') +
  xlim(c(1.5, 4)) +
  theme_clean() +
  theme(
    axis.text.x = element_text(size = 8),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.key.size = unit(2, 'mm'),
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 8),
    legend.box.spacing = unit(0, 'in'),
    legend.position = c(.85, .75),
    title = element_text(size = 12)
  )

gslopes = tibble(
  Mixed = extract_random_coefs(gpa_mixed) %>% filter(effect == 'occasion') %>% pull(value),
  Separate = gpa_lm_by_group %>% pull(occasion)
) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'Occasion') %>%
  ggplot(aes(x = Occasion)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(x = '', y = '', title = 'Slopes for occasion') +
  xlim(c(-.2, .4)) +
  theme_clean() +
  theme(
    axis.text.x = element_text(size = 8),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    title = element_text(size = 12)
  )

gint + gslopes

ggsave('img/shrinkage_main.svg')
```

```{r shrinkage_main, out.width='125%', echo = F}
knitr::include_graphics('img/shrinkage_main.svg')
```

여기에서 우리는 mixed model 절편이 일반적으로 극단적이지 않음을 볼 수 있다, 즉, 분포 꼬리가 전체 효과쪽으로 당겨졌다. 
기울기들도 마찬가지이다. 두 경우다 mixed model 은 by-group 추정치를 감소시켰는데, 다르게 취급했다면 이러한 시나리오에서 과적합되었을 것이다. 이러한 <span class="emph">regularizing</span> 효과는 mixed model[^pool]을 사용할 때 얻을 수 있는 또다른 보너스이다. 그룹마다 관측값이 많지 않고, 랜덤이팩트의 추정 분산이 작을 때 이러한 일이 일어난다. 다른 말로 하면, 그룹마다 정보가 적거나, 관측값 분산에 비해 그룹레벨 분산이 상대적으로 작으면, mixed model 은 group-specific effect 가 전체 population effect 에 가까운 값으로 제공할 것이다. 이러한 맥락에서 mixed model group 계수들은 우리가 모른다는 것을 더 잘 반영한다. 반대로, with more pronounced group effec, 전체 효과에 대한 우리의 불확실성은 증가한다.

다음은 앞서 보았던 GPA 모델의 결과에 기반한 시뮬레이션 데이터의 다양한 세팅 아래에서 비슷한 데이터에 어떤 일들이 일어나는 지를 보여준다. 맨 왼쪽은 현재 데이터 세팅을 바로 보여준다. 그 다음, 원 결과를 따라 네가지 세팅이 있다. 처음은 학생마다 훨씬 많은 측정값이 있다면 어떤 일이 일어나는지를 보여준다. 다음은, 절편과 기울기 분산을 증가하고 잔차분산을 감소시키지만 샘플 사이즈를 원 데이터와 같게 유지한다. 두 경우 다 mixed model 의 regulizaing effect 가 더 작다. 랜덤 계수들은 개별 회귀 결과와 매우 유사하다. 그리고, 데이터를 동일하게 유지하지만, 학생당 4 관측값만 있을 때, 학생당 결과에서 변동성이 더 커서 mixed model 에서 shrinkage 가 상대적으로 덜 일어난다. 마지막으로 학생당 occasions 숫자를 더하지만 (10), 시간이 흐르면서 유실도 있어서 대략 데이터 양은 비슷하지만, 불균형이다. 이 주제에 대한 더 보고 싶으면, 내 게시글을  [여기](https://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/) 에서 보라.



```{r ranints_vs_separateints_more_npg, echo=FALSE, eval=FALSE}
set.seed(1234)
Nstudent = 200
NperGroup = 100
N = Nstudent * NperGroup

student = factor(rep(1:Nstudent, each = NperGroup))
u = mvtnorm::rmvnorm(Nstudent, sigma = matrix(c(.2 ^ 2,-.1 * .2 * .067,-.1 *
                                                  .2 * .067, .067 ^ 2), 2, 2))
e = rnorm(N, sd = .25)
occasion = rep(0:(NperGroup - 1), Nstudent)
y = (2.6 + u[student, 1]) + (.11 + u[student, 2]) * occasion + e

d = data.frame(occasion, y, student)

model = lmer(y ~ occasion + (1 + occasion | student), data = d)

separate_lm = d %>% 
  split(.$student) %>% 
  map_df(~data.frame(t(coef(lm(y ~ occasion, data=.))))) %>% 
  rename(Intercept = X.Intercept.)

gint_more_npg = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'Intercept') %>% pull(value),
  Separate = separate_lm %>% pull(Intercept)
) %>%
  pivot_longer(everything(),  names_to= 'Model', values_to = 'Intercept') %>% 
  ggplot(aes(x = Intercept)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(x = '', y = '', title = 'Intercepts', subtitle = 'More observations per group') +
  xlim(c(1, 4)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    legend.key.size    = unit(2, 'mm'),
    legend.title       = element_text(size = 8),
    legend.text        = element_text(size = 8),
    legend.box.spacing = unit(0, 'in'),
    legend.position    = c(.75, .75),
    title = element_text(size = 7)
  )

gslopes_more_npg = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'occasion') %>% pull(value),
  Separate = separate_lm %>% pull(occasion)
) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'occasion') %>%
  ggplot(aes(x = occasion)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(x = '', y = '', title = 'Slopes for occasion', subtitle = 'More observations per group') +
  xlim(c(-.25, .4)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    title = element_text(size = 7)
  )

# gint_more_npg + gslopes_more_npg
```

```{r ranints_vs_separateints_more_re, echo=FALSE, eval=FALSE}
set.seed(1234)
Nstudent  = 200
NperGroup = 6
N = Nstudent * NperGroup

student = factor(rep(1:Nstudent, each = NperGroup))
u = mvtnorm::rmvnorm(Nstudent, sigma = matrix(c(.4 ^ 2,-.1 * .4 * .2,-.1 *
                                                  .4 * .2, .2 ^ 2), 2, 2))
e = rnorm(N, sd = .15)
occasion = rep(0:(NperGroup - 1), Nstudent)
y = (2.6 + u[student, 1]) + (.11 + u[student, 2]) * occasion + e

d = data.frame(occasion, y, student)

model = lmer(y ~ occasion + (1 + occasion |student), data=d)

separate_lm = d %>% 
  split(.$student) %>% 
  map_df(~data.frame(t(coef(lm(y ~ occasion, data=.))))) %>% 
  rename(Intercept = X.Intercept.)

gint_more_re = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'Intercept') %>% pull(value),
  Separate = separate_lm %>% pull(Intercept)
) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'Intercept') %>%
  ggplot(aes(x = Intercept)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(
    x = '',
    y = '',
    title = 'Intercepts',
    subtitle = 'More RE variance'
  ) +
  xlim(c(1, 4)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    legend.key.size    = unit(2, 'mm'),
    legend.title       = element_text(size = 8),
    legend.text        = element_text(size = 8),
    legend.box.spacing = unit(0, 'in'),
    legend.position    = c(.75, .75),
    title = element_text(size = 7)
  )

gslopes_more_re = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'occasion') %>% pull(value),
  Separate = separate_lm %>% pull(occasion)
) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'occasion') %>%
  ggplot(aes(x = occasion)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(
    x = '',
    y = '',
    title = 'Slopes for occasion',
    subtitle = 'More RE variance'
  ) +
  xlim(c(-.6, 1)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    title = element_text(size = 7)
  )

# gint_more_re + gslopes_more_re
```

```{r ranints_vs_separateints_less_npg, echo=FALSE, eval=FALSE}
set.seed(1234)
Nstudent = 200
NperGroup = 3
N = Nstudent * NperGroup

student = factor(rep(1:Nstudent, each = NperGroup))
u = mvtnorm::rmvnorm(Nstudent, sigma = matrix(c(.2 ^ 2,-.1 * .2 * .067,-.1 *
                                                  .2 * .067, .067 ^ 2), 2, 2))
e = rnorm(N, sd = .25)
occasion = rep(0:(NperGroup - 1), Nstudent)
y = (2.6 + u[student, 1]) + (.11 + u[student, 2]) * occasion + e

d = data.frame(occasion, y, student)

model = lmer(y ~ occasion + (1 + occasion |student), data=d)

separate_lm = d %>% 
  split(.$student) %>% 
  map_df(~data.frame(t(coef(lm(y ~ occasion, data=.))))) %>% 
  rename(Intercept = X.Intercept.)

gint_less_npg = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'Intercept') %>% pull(value),
  Separate = separate_lm %>% pull(Intercept)
  ) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'Intercept') %>%
  ggplot(aes(x = Intercept)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(
    x = '',
    y = '',
    title = 'Intercepts',
    subtitle = 'Fewer observations per group'
  ) +
  xlim(c(1, 4)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    legend.key.size    = unit(2, 'mm'),
    legend.title       = element_text(size = 8),
    legend.text        = element_text(size = 8),
    legend.box.spacing = unit(0, 'in'),
    legend.position    = c(.75, .75),
    title = element_text(size = 7)
  )

gslopes_less_npg = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'occasion') %>% pull(value),
  Separate = separate_lm %>% pull(occasion)
  ) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'occasion') %>%
  ggplot(aes(x = occasion)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(
    x = '',
    y = '',
    title = 'Slopes for occasion',
    subtitle = 'Fewer observations per group'
  ) +
  xlim(c(-.6, .8)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    title = element_text(size = 7)
  )

# gint_less_npg + gslopes_less_npg
```

```{r ranints_vs_separateints_imbalanced, echo=FALSE, eval=FALSE}
set.seed(1234)  
Nstudent = 200
NperGroup = 10
N = Nstudent * NperGroup

student = factor(rep(1:Nstudent, each = NperGroup))
u = mvtnorm::rmvnorm(Nstudent, sigma = matrix(c(.2 ^ 2, -.1 * .2 * .067, -.1 *
                                                  .2 * .067, .067 ^ 2), 2, 2))
e = rnorm(N, sd = .25)
occasion = rep(0:(NperGroup - 1), Nstudent)
y = (2.6 + u[student, 1]) + (.11 + u[student, 2]) * occasion + e

d = data.frame(occasion, y, student)
d = d %>%
  group_by(student) %>%
  slice(c(1, sample(
    2:10,
    size = sample(1:9,
                  prob = c(.95, .95, .9, .9, .85, .85, rep(.8, 3))),
    prob = c(.95, .95, .9, .85, .80, .75, .7, .6, .5)
  ))) %>% 
  arrange(occasion, .by_group = TRUE)

model = lmer(y ~ occasion + (1 + occasion |student), data=d)

separate_lm = d %>% 
  split(.$student) %>% 
  map_df(~data.frame(t(coef(lm(y ~ occasion, data=.))))) %>% 
  rename(Intercept = X.Intercept.)


gint_imbalanced = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'Intercept') %>% pull(value),
  Separate = separate_lm %>% pull(Intercept)
  ) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'Intercept') %>%
  ggplot(aes(x = Intercept)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(
    x = '',
    y = '',
    title = 'Intercepts',
    subtitle = 'Imbalanced'
  ) +
  xlim(c(1, 4)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y  = element_blank(),
    axis.ticks.y = element_blank(),
    legend.key.size    = unit(2, 'mm'),
    legend.title       = element_text(size = 8),
    legend.text        = element_text(size = 8),
    legend.box.spacing = unit(0, 'in'),
    legend.position    = c(.75, .75),
    title = element_text(size = 7)
  )

gslopes_imbalanced = tibble(
  Mixed = extract_random_coefs(model) %>% filter(effect == 'occasion') %>% pull(value),
  Separate = separate_lm %>% pull(occasion)
) %>%
  pivot_longer(everything(),  names_to = 'Model', values_to = 'occasion') %>%
  ggplot(aes(x = occasion)) +
  geom_density(aes(color = Model, fill = Model),
               alpha = .25,
               show.legend = F) +
  scale_fill_manual(values = c(palettes$orange$orange, palettes$orange$complementary[2])) +
  labs(
    x = '',
    y = '',
    title = 'Slopes for occasion',
    subtitle = 'Imbalanced'
  ) +
  xlim(c(-.6, .8)) +
  theme_clean() +
  theme(
    axis.text.x  = element_text(size = 6),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    title = element_text(size = 7)
  )

# gint_imbalanced + gslopes_imbalanced
```



```{r all_together_now, echo=F, fig.align='left', out.width='100%', eval=F}
# all previous chunks need to be run 

(gint + theme(title = element_text(size = 8)) | gint_more_npg | gint_more_re | gint_less_npg | gint_imbalanced) /
  (gslopes  + theme(title = element_text(size = 8))| gslopes_more_npg | gslopes_more_re | gslopes_less_npg | gslopes_imbalanced)

ggsave('img/shrinkage_all.svg')
```

```{r shrinkage_all, out.width='125%', echo = F}
knitr::include_graphics('img/shrinkage_all.svg')
```


## 효과 시각화

우리 예측값들을 시각적으로 비교해보자. 첫번째로 선형 회귀 적합이 있다. 시작점과 기울기가 모든 사람에게 있어 같다고 가정하자. 우리가 mixed model 에 subject specific 효과가 있는 조건부 예측을 더한다면, subject specific 예측을 할 수 있어서 모델의 실용성을 크게 증가시킨다. 


```{r visualize_mixed_fit, echo=F, eval=-1}
going_down_now = factor(rep(coef(gpa_mixed)$student[, 'occasion'] < 0, e =6), 
                        labels = c('Up', 'Down'))

gpa %>%
  modelr::add_predictions(gpa_lm, var = 'lm') %>%
  modelr::add_predictions(gpa_mixed, var = 'mixed') %>%
  group_by(student) %>%
  plot_ly %>%
  add_lines(
    x =  ~ occasion,
    y =  ~ lm,
    opacity = 1,
    color = I('#ff5500'),
    name = 'Standard\nRegression'
  ) %>%
  add_lines(
    x =  ~ occasion,
    y =  ~ mixed,
    opacity = .2,
    color = I('#00aaff'),
    name = 'Mixed\nModel'
  ) %>%
  layout(yaxis = list(title = 'gpa')) %>%
  theme_plotly()
```

<br>

반면에 그룹별 접근법은 모든 사람을 독립적으로 다루기 때문에 더 노이지하다. 학생이 더 많으면 mixed model 대비 기울기가 아래를 향하거나 평평해질 것이다. 하지만, mixed model 은 `r sum(coef(gpa_mixed)$student[,'occasion']<0)` 개의 기울기만 음수로 추정이 된다. 

```{r visualize_bygroup_fit, echo=FALSE, cache=FALSE}
gpa_lm_fits_by_group = gpa %>% 
  split(.$student) %>% 
  map(~lm(gpa ~ occasion, data=.x)) %>% 
  map(fitted) %>% 
  unlist

going_down_now = factor(rep(gpa_lm_by_group[,'occasion']<0, e=6), 
                        labels=c('Upward', 'Downward'))

# plotly actually ignores the colorscale argument for the second trace; it also doesn't know what to do with alpha hex, nor what opacity means
gpa %>%
  modelr::add_predictions(gpa_lm, var = 'gpa') %>%
  mutate(stufit = gpa_lm_fits_by_group) %>%
  group_by(student) %>%
  plot_ly(x =  ~ occasion, y =  ~ stufit) %>%
  add_lines(
    color =  ~ going_down_now,
    colors = scico::scico(
      2,
      begin = .4,
      end = .75,
      palette = 'oleron'
    ),
    opacity = .2
  ) %>%
  add_lines(
    x =  ~ occasion,
    y =  ~ gpa,
    opacity = 1,
    color = I(palettes$orange$orange),
    name = 'Standard\nRegression'
  ) %>%
  layout(yaxis = list(title = 'gpa')) %>%
  theme_plotly()
```


## 요약

이러한 것들을 왜 *richly parameterized linear models* 로 불리는지 감을 잡았을 것이다. 표준 회귀에 비해 모델의 불확실성 소스에 대한 우리의 이해에 더해지는 추가 분산 파라미터를 얻고, subject specific 효과와 correlation 을 얻고, 이 정보를 이용하여 훨씬 좋은 예측값을 얻을 수 있다.  좋아하지 않을 이유가 없다.

## Exercises for Random Slopes

#### Sleep revisited

Run the sleep study model with random coefficient for the Days effect, and interpret the results.  What is the correlation between the intercept and Days random effects?  Use the <span class="func">ranef</span> and <span class="func">coef</span> functions on the model you've created to inspect the individual-specific effects. What do you see?

```{r sleepstudy2}
library(lme4)
data("sleepstudy")
```

In the following, replace <span class="objclass">model</span> with the name of your model object. Run each line, inspecting the result of each as you go along. 

```{r ex-sleep, eval=FALSE}
re = ranef(model)$Subject
fe = fixef(model)

apply(re, 1, function(x) x + fe) %>% t()
```

The above code adds the fixed effects to each row of the random effects (the <span class="func">t</span> just transposes the result). What is the result compared to what you saw before?



#### Simulation revisited

The following shows a simplified way to simulate some random slopes, but otherwise is the same as the simulation before.  Go ahead and run the code.

```{r simSlopes, eval=FALSE}
set.seed(1234)  # this will allow you to exactly duplicate your result
Ngroups = 50
NperGroup = 3
N = Ngroups * NperGroup
groups = factor(rep(1:Ngroups, each = NperGroup))
re_int = rnorm(Ngroups, sd = .75)
re_slope = rnorm(Ngroups, sd = .25)
e = rnorm(N, sd = .25)
x = rnorm(N)
y = (2 + re_int[groups]) + (.5 + re_slope[groups]) * x + e

d = data.frame(x, y, groups)
```

This next bit of code shows a way to run a mixed model while specifying that there is no correlation between intercepts and slopes.  There is generally no reason to do this unless the study design warrants it[^nocorr], but you could do it as a step in the model-building process, such that you fit a model with no correlation, then one with it.

```{r simSlopes2, eval=FALSE}
model_ints_only = lmer(y ~ x + (1|groups), data = d)

model_with_slopes = lmer(y ~ x + (1|groups) + (0 + x|groups), data = d)

summary(model_with_slopes)

confint(model_with_slopes)

library(ggplot2)

ggplot(aes(x, y), data = d) +
  geom_point()
```

Compare model fit using the <span class="func">AIC</span> function, e.g. `AIC(model)`.  The model with the lower AIC is the better model, so which would you choose?


[^nointneeded]: Technically the intercept is assumed but you should keep it for clarity.

[^sdslopes]: In case it's not clear, I'm using the fact that we assume a normal distribution for the random effect of occasion.  A quick rule of thumb for a normal distribution is that 95% falls between $\pm$ 2 standard deviations of the mean.

[^nocorr]: I personally have not come across a situation where I'd do this in practice.  Even if the simpler model with no correlation was a slightly better fit, there isn't much to be gained by it.

[^fe_comparison]: This is just one issue with a fixed effects approach. You would have to estimate 400 parameters, but without anything (inherently) to guard against overfitting.  The so-called *fixed effects model* from the econometrics perspective gets around this by demeaning variables that vary within groups, i.e. subtracting the per group mean. This is also equivalent to a model adding a dummy variable for the groups, though it's a computationally more viable model to fit, as one no longer includes the grouping variable in the model (not really a concern with data FE models are actually applied to and it being what year it is).  But while it controls for group level effects, we still cannot estimate them. Traditional approaches to fixed effects models also do not have any way to incorporate group-specific slopes, except perhaps by way of an interaction of a covariate with the cluster, which brings you back to square one of having to estimate a lot of parameters.  For more about FE models and their issues, see my document on [clustered data approaches](https://m-clark.github.io/clustered-data/fixed-effects-models.html), and Bell et al. (2016). Fixed and Random effects: making an informed choice.

```{r plm, echo=FALSE, eval=FALSE}
plm_model = plm::plm(gpa ~ occasion, model = 'within', data=gpa)
summary(plm_model)
wi = plm::fixef(plm_model) 

gpa = gpa %>% 
  group_by(student) %>% 
  mutate(gpa_demean=gpa-mean(gpa), 
         occasion_demean=occasion-mean(occasion))
summary(lm(gpa_demean ~ occasion_demean - 1, gpa))
# summary(lm(gpa ~ 0 + occasion + student, gpa))
# gpa_mixed = lmer(gpa ~ occasion + (1|student), data=gpa)
# head(cbind(gpa_lm_by_group[,1], wi, coef(gpa_mixed)[[1]]))

tibble(
  `Mixed model` = coef(gpa_mixed)$student[, 1],
  `Fixed effects model` = wi,
  `LM by group` = ints_fixed
) %>%
  gather(key = Model, value = Intercept) %>%
  ggplot(aes(x = Intercept)) +
  geom_density(aes(color = Model, fill = Model), alpha = .25) +
  scale_fill_manual(
    values = c(
      palettes$orange$orange,
      palettes$orange$complementary[2],
      palettes$orange$tetradic[3]
    )
  ) +
  ggtitle('Intercepts') +
  labs(x = '', y = '') +
  xlim(c(1.5, 4)) +
  theme_clean() +
  theme(
    legend.key.size = unit(2, 'mm'),
    legend.title = element_text(size = 8),
    legend.text = element_text(size = 8),
    legend.box.spacing = unit(0, 'in'),
    legend.position = c(.75, .75)
  )
```


[^pool]: This phenomenon is also sometimes referred to as <span class="emph">partial pooling</span>.  This idea of pooling is as in 'pooling resources' or borrowing strength.  You have complete pooling, which would be the standard regression model case of ignoring the clusters, i.e. all cluster effects are assumed to be the same. With no pooling, we assumes the clusters have nothing in common, i.e. the separate regressions approach.  Partial pooling is seen in the mixed model scenario, where the similarity among the clusters is estimated in some fashion, and data for all observations informs the estimates for each cluster.  I've never really liked the 'pooling' terminology, as regularization/shrinkage is a more broad concept that applies beyond mixed models, and I'd prefer to stick to that.  In any case, if interested in more, see practically anything Andrew Gelman has written on it, and the pool-no-pool document [here](https://github.com/stan-dev/example-models/tree/master/knitr/pool-binary-trials).
