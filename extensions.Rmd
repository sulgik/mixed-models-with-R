```{r chunk_setup-ext, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(
  # cache
  cache.rebuild = F,
  cache         = T
)
```


# 확장모델

<br>

```{r top-plot-crossed, echo=FALSE, cache.rebuild=TRUE}
tags$div(
  style = "width:50%; margin:auto auto; font-size:50%",
  DiagrammeR::grViz(
    'scripts/crossed.gv', 
    width = '100%', 
    height = '33%'
  )
)
```

<br>

## 추가 군집 구조


### Cross-classified models

하나의 군집 요인을 넘어 추가적인 분산 소스가 있는 경우가 종종 생길 것이다. 예를들어, 특정 그림들을 보여주고 각 개인이 여러번 시행하는 시각인지 실험을 생각해보자. 데이터가 다음과 같을 것이다. 

```{r demodata_crossed, echo=FALSE}
crossing(Person = 1:20, Image = letters[1:10]) %>%
  mutate(score = sample(1:10, 200, replace = T)) %>%
  DT::datatable(
    options = list(
      dom = 'tp',
      autoWidth = F,
      columnDefs = list(
        list(width = '10px', targets = 0:2),
        list(className = 'dt-center', targets = 0:2)
      )
    ),
    rownames = F,
    width = 250
  )
```

<br>
<br>

이 경우, 관측값이 사람과 이미지 내로 군집이 되지만, 사람과 이미지는 서로 포함관계가 아니고 - 모든 참가자들은 10개 이미지 모두를 보았다. 이러한 경우는 일반적으로 <span class="emph">crossed</span> random effect 가 있다라고 하는데, non-nested 라는 의미이다. 다음에서 보게 될 상황에서 여러개의 소스의 분산을 보게 될 것이다.


#### 예: 학생 학업성취도

예시를 위해 학생의 학업성취도 점수를 살펴볼 것이다. 의존성 소스는 같은 초등학교를 가거나, 중학교 진학한 학생에 기인한다. 하지만, 이 예에서, 한 초등학교(primary school)를 간다는 것은 반드시 특정한 중학교(secondary school)를 가는 것은 아니다. 반복측정값이 없고, 각 학생을 딱 한번만 측정했다는 것을 주목하라. 데이터를 빠르게 본 것인데, 더 자세한 내용은 [appendix][Data] 을 확인하라.


```{r pupil_nurses_setup, echo=FALSE, eval=FALSE}
pupils = read_sav('data/raw_data/joop_hox_data2/9 CrossClass/pupcross.sav') %>%
  as_factor() %>%
  mutate(ACHIEV = as.numeric(as.character(ACHIEV)),
         PUPSEX = factor(PUPSEX, labels = c('male', 'female'))) %>%
  rename(
    achievement = ACHIEV,
    primary_school_id = PSCHOOL,
    secondary_school_id = SSCHOOL,
    sex = PUPSEX,
    ses = PUPSES,
    primary_denominational = PDENOM,
    secondary_denominational = SDENOM
  )  

save(pupils, file='data/pupils.RData')

nurses = read_sav('data/raw_data/joop_hox_data2/2 Basic Model/nurses.sav') %>%
  as_factor() %>%
  rename(experience = experien,
         treatment = expcon,
         sex = gender) %>%
  mutate(
    treatment = factor(treatment, labels = c('Ctrl', 'Training')),
    age = as.numeric(as.character(age)),
    sex = factor(sex, labels = c('Male', 'Female'))
  ) %>%
  select(-starts_with('Z'),-starts_with('C'))

save(nurses, file = 'data/nurses.RData')
```


```{r examine_pupil_data, echo=1}
load('data/pupils.RData')

DT::datatable(pupils,
              options = list(
                dom = 'tp',
                scrollX = T,
                autoWidth = T
              ),
              # columnDefs = list(list(width = '150px', targets = 1),
              #                   list(width = '100px', targets = 3))),
              rownames = F)
```
<br>
<br>

우리의 mixed model 에서, 학업성취도에 대한 `sex`, 낮음에서 높음까지의 여섯 단계 변수인 사회경제적 지위, `ses` 효과를 살펴볼 것이다. 성취도 범위는 약 `r round(min(pupils$achievement))` 에서 `r round(max(pupils$achievement))` 까지이고, 평균은 `r round(mean(pupils$achievement), 1)`, 표준편차는 `r round(sd(pupils$achievement), 1)` 이다. 초등학교와 중학교의 클러스터링 고려할 것이다. 지금은 그룹 인자가 두 개[^crossed_notation]라는 것만 제외하면, <span class="pack">lme4</span> 문법으로 추가 구조를 넣는 것은 앞에서 본 것 처럼 매우 쉽다.

```{r cross_classified, eval=1}
pupils_crossed = lmer(
  achievement ~ sex + ses 
  + (1|primary_school_id) + (1|secondary_school_id),
  data = pupils
)

summary(pupils_crossed, correlation = FALSE)
```


```{r cross_classified_fixed, echo=FALSE}
pupils_crossed %>% 
  extract_fixed_effects() %>% 
  select(-(t:p_value)) %>% 
  kable_df()
```

fixed effect 는 학업성취도에 있어 여자인 것이 양의 효과를 가지고, 일반적으로, 가장 낮은 사회경제적지위 범주에 비해 상위 범주가 양의 효과를 가진다는 것을 보여준다.

```{r  cross_classified_random, echo=FALSE}
crossed_var_cor = extract_vc(pupils_crossed, ci_level = 0) 

crossed_var_cor %>%
  kable_df(digits = 2)
```

분산 컴포넌트를 보면, 초등학교와 중학교가 전체 변동의 약 `r perc(sum(crossed_var_cor$variance[1:2])/sum(crossed_var_cor$variance), digits = 0)` 를 기여하고 있음을 알 수 있다. 학교에 기인한 변동의 대부분은 초등학교에서 온다.

랜덤효과를 조사하면, 두가지 세트의 효과를 가진다는 것을 볼 수 있다 - 초등학교에 해당하는 `r n_distinct(pupils$primary_school_id)` 개와 중학교에 해당하는 `r n_distinct(pupils$secondary_school_id)` 개 이다. 두 개다 학생-specific 예측에 포함된다.

```{r crossed_re}
glimpse(ranef(pupils_crossed))
```

<span class="pack">merTools</span> 을 사용하여 시각적으로 살펴보자.

```{r crossed_re_plot, echo=FALSE}
p = merTools::plotREsim(merTools::REsim(pupils_crossed)) +
  theme_clean()
p

# multiplots with patchwork still do non-transparent background
# library(patchwork)
# p1 = plot_coefficients(pupils_crossed, ranef=T, which_ranef='primary_school_id') +
#   labs(x='Primary School') +
#   lims(y = c(-1.5,1.5)) +
#   theme(axis.ticks = element_blank(),
#         axis.text.x = element_blank())
# p2 = plot_coefficients(pupils_crossed, ranef=T, which_ranef='secondary_school_id') +
#   labs(x='Secondary School', y='') +
#   lims(y = c(-1.5,1.5)) +
#   theme(axis.ticks = element_blank(),
#         axis.text.x = element_blank(),
#         axis.text.y = element_blank())
# {p1 + p2} 
```

우리는 원하는대로 모델을 확장할 수 있다는 것을 주목하라. 예를 들어, 학생레벨 특징에 대해 랜덤 기울기를 할 수도 있다.

### 계층구조

cross-classified 모델을 살펴보았고 이제 계층적 클러스터 구조화하는 것으로 나아갈 수 있다. 이 경우, 클러스터들이 다른 클러스터 내부에 포함되고, 다른 클러스터도 다른 클러스터 내부에 포함되는 경우가 있다. 쉬운 예는 행정시 내부의 행정동과, 행정'도' 내부의 행정'시'들이다. 

#### 예: 간호사와 스트레스

실례를 위해 간호사 데이터셋을 사용할 것이다. 교육프로그램 (`treatment`) 이 간호사의 스트레스 (1-7 점 점수) 에 주는 효과에 관심이 있다. 이 시나리오에서, 간호사는 병동 내에 포함되어있는데, 병동은 병원에 포함되어 있기 때문에 병동과 병원에 해당하는 랜덤효과를 가질 것이다. 더 자세한 내용은 [appendix][Data] 를 참고하라.

```{r nurses_data, echo=1}
load('data/nurses.RData')

DT::datatable(nurses,
              options = list(
                dom = 'tp',
                scrollX = T,
                autoWidth = T
              ),
              # columnDefs = list(list(width = '150px', targets = 1),
              #                   list(width = '100px', targets = 3))),
              rownames = F)
```

<br>
<br>

이 모델의 처치 (treatment) 효과와 간호사, 병동, 병원 수준 각각 하나를 가지는 공변량을 측정한다. fixed effect 에 관해서는 다른 표준 회귀와 같이 이론/탐색법이 제안하는대로 공변량을 추가한다. 이러한 랜덤효과를 구현하는 것은 cross-classified 접근법과 크게 다르지 않지만 문법에 약간 다른 점이 있다.

```{r hierarchical, eval=1}
nurses_hierarchical = lmer(
  stress ~ age + sex + experience + treatment + wardtype + hospsize
  + (1 | hospital) + (1 | hospital:ward),
  data = nurses
)

# same thing!
nurses_hierarchical = lmer(
  stress ~ age  + sex + experience + treatment + wardtype + hospsize 
  + (1|hospital/ward), 
  data = nurses
) 

summary(nurses_hierarchical, correlation = F)
```

```{r hierarchical_fixed, echo=FALSE}
nurses_hierarchical %>% 
  extract_fixed_effects() %>% 
  select(-(t:p_value)) %>% 
  kable_df()
```

<br>

fixed 효과에 관해서는, 통계적인 효과가 없는 것은 병동유형(wardtype)이다 [^signflip].  

<br>

```{r hierarchical_random, echo=FALSE}
# note tidy doesn't work with multiple random effects and conf.int
hierarch_var_cor = extract_vc(nurses_hierarchical, ci_level = 0)

hierarch_var_cor %>%
  kable_df()
```

랜덤효과에 관해서는, 병동 병동마다 변동성이 있고, 병원도 그렇다. 스트레스는 7 점 스케일이기 때문에 병동 병동마다 평균 약 0.5 점 정도 튈것으로 생각되는데, 이는 내 의견에는 꽤 극적이다. 시각적으로 살펴본다.

```{r hierarchical_re_plot, echo=FALSE}
p = merTools::plotREsim(merTools::REsim(nurses_hierarchical)) +
  theme_clean()
p
```


### Crossed vs. nested

다음은 병동을 (병원 내) nested 로 처리하는것 대비 crossed 랜덤효과로 처리하는것의 차이점을 보여준다. 뭐가 다른 점으로 보이는가?

```{r crossed_vs_nested, echo=1:3}
nurses_hierarchical = lmer(
  stress ~ age  + sex + experience + treatment + wardtype + hospsize 
  + (1|hospital) + (1|hospital:wardid), 
  data = nurses
)

nurses_crossed = lmer(
  stress ~ age  + sex + experience + treatment + wardtype + hospsize 
  + (1|hospital) + (1|wardid),
  data = nurses
)

hierarch_var_cor %>%  
  kable_df()

crossed_var_cor = extract_vc(nurses_crossed, ci_level = 0)

crossed_var_cor %>%  
  kable_df()
```

없다? 좋다, 당신이 이상한 것이 아니다. 다음은 [lme4 text](http://lme4.r-forge.r-project.org/book/Ch2.pdf), section 2.2.1.1, 의 글인데, 시간을 써서 읽을 가치가 있다.

> 혼합효과 모델과 다중 계층 수준을 헷갈리는 것은 모델을 정의할 때 수준에 보장되지 않는 강조를 낳고, 상당한 혼란을 야기한다. non-nested 요인과 관련된 랜덤 효과를 갖는 모델을 정의하는 것은 완벽하게 정당하다. nested 요인에 관해서만 랜덤효과를 정의하는 것을 강조하는 이유는 이러한 경우가 실생활에서 자주 일어나고 모델의 파라미터를 추정하는 계산법이 nested 요인에 쉽게 적용할 수 있기 때문이다.
> 
이는 lme4 패키지에서 사용한 방법의 경우는 아니다. *사실 nested 요인에 랜덤효과를 주는 모델은 특별한 것이 없다*. 랜덤효과가 다중 요인과 연관될때, 요인들이 nested sequence 를 형성하는지, 부분적으로 crossed 인지, 완벽히 crossed 와 상관없이 같은 계산법이 사용된다.

첫 예에서 그룹변수로 `ward` 가 아닌 `wardid` 를 사용했다는 것을 눈치챘는가? 모든 병동이 유일하지만, `ward` 열은 column labels them with an arbitrary sequence starting with 1 부터 시작하는 임의의 sequence 로 라벨링한다.  이것이 자연스러워 보일 수 있지만, hospital 1 의 ward 1 은 hospital 2 의 ward 1 과 같은 병동이 아니기 때문에 이들에게 같은 라벨을 주는 것은 좋은 방법이 아닐것이다. `wardid` 열은 적절하게 교유 값으로 병동들을 구분한다 (예. 11, 12).

이 변수를 crossed random effect 로 사용한다면 어떤 일이 벌어질까?

```{r bad_cross, echo=1}
nurses_crossed_bad_data = lmer(
  stress ~ age  + sex + experience + treatment + wardtype + hospsize 
  + (1|hospital) + (1|ward), 
  data = nurses
)

extract_vc(nurses_crossed_bad_data, ci_level = 0) %>%
  kable_df()
```

아마 우리가 원하는 결과가 절대 아니다. `ward` 분산은 이미 treatment 와 type 에 의해 캡쳐됐다. 하지만, 보았듯이, 적절한 문법을 사용하거나, 고유 클러스터를 고유한 식별자를 갖도록 하는, 데이터내에서의 적절한 라벨링을 하면 피할 수 있다.

이 [논의](https://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified) 와 <span class="pack">lme4</span> 개발자 중 한명이 쓴 [FAQ](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#nested-or-crossed) 를 읽으라. CSCAR 의 Josh Errickson 도 관심있는 행렬 [시각화에 관한 글](http://errickson.net/stats-notes/vizrandomeffects.html)을 썻는데, 다음 섹션의 시각화의 동기가 되었다.

자 이제 되었다. <span class="pack">lme4</span>, 더 일반적으로 mixed models, crossed vs. nested 는 단순히 (data) 에 관한 생각을 어떻게 하느냐에 달려있다 [^crossnest].


## 잔차 구조

잔차 공분산/상관관계 구조와 관련하여 더 구체적인 값을 측정할 필요가 있는 경우가 있다. longitudinal 세팅에서 특별히 그러한데, 이 세팅에서는 관측값이 시간상 가까울 수록 먼 것보다 강한 상관관계가 있거나 분산이 시간에 따라 변한다고 생각한다. 이 모델은 어떻게 생겼을까?

우선, 우리 타겟 변수의 전체 관측값의 분산/상관 행렬과, 이러한 관측값에서 dependency 를 어떻게 표현하는 것이 좋을지를 생각해보자. 우리 GPA 데이터와 모델링 상황에 대해 첫 5 명의 시각화를 볼 것이다. 


각 사람은 6개의 관측값이 있다는 것을 기억하라. GPA 에 관한 우리의 랜덤 절편 (only) 모델의 결과를 보여준다.

```{r residual_varcov, echo=FALSE}
rescov <- function(model, data) {
  var.d <- crossprod(getME(model,"Lambdat"))
  Zt <- getME(model,"Zt")
  vr <- sigma(model)^2
  var.b <- vr*(t(Zt) %*% var.d %*% Zt)
  sI <- vr * Diagonal(nrow(data))
  var.y <- var.b + sI
  invisible(var.y)
}
gpa_vis_cov = lmer(gpa ~ occasion + (1|student), data=gpa)
# summary(gpa_vis_cov)

rc1 <- rescov(gpa_vis_cov, gpa)
# image(rc1[1:60,1:60])
rc1[1:30, 1:30] %>%
  as.matrix() %>%
  reshape2::melt() %>%
  mutate(value = factor(round(value, 5))) %>%
  ggplot(aes(Var1, Var2, fill = value)) +
  geom_tile(color = 'gray92', size = 1) +
  scale_y_continuous(trans = 'reverse') +
  scale_fill_manual(values = c('gray92', '#00aaff80', '#ff550080')) +
  theme_void() +
  theme(
    legend.key = ggplot2::element_rect(fill = 'transparent', colour = NA),
    legend.background = ggplot2::element_rect(fill = 'transparent', colour = NA),
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    strip.background = ggplot2::element_blank(),
    plot.background = ggplot2::element_rect(fill = "transparent", colour = NA)
  )

  
# ggplotly()

# library(plotly)
# library(viridis)
# plot_ly(z =~ as.matrix(rc1[1:30, 1:30]), type='contour', colors = c('gray92','#00aaff80', '#ff550080')) %>%
#   layout(yaxis = list(autorange = "reversed"))
```

각 블락은 한 개인 내 관측값들에 해당하는 공분산 행렬을 나타낸다. 해당 사람 내에서 대각선에 분산들이 있고 대각선바깥에는 공분산들이 있다. 모든 데이터를 고려할 때, 한 사람의 관측값들은 다른 사람과 공분산이 없다는 것을 알 수 있다 (회색). 더 나아가서, 한 사람 내 공분산은 상수값이고, 분산도 또한 상수 값이다. 이 값들은 어디서 왔을까?

```{r residual_varcov2, echo=F}
vc = extract_vc(gpa_vis_cov, ci_level = 0)

totvar = sum(vc$variance)

vc %>%  
  kable_df()
```

이 모델에서 두개의 분산 소스가 있고, 잔차 관측값 수준 분산과 사람에 관련된 것이라는 것을 기억하라. 이 두 분산이 합쳐져서, 우리 공변량으로 설명하지 않은 총 잔차 분산을 제공한다. 우리의 경우, 약 `r round(totvar, 2)` 이며 우리 대각선에 표현된 값이다. 대각이 아닌 곳은 학생에 관한 분산인데, 다른 시각으로는 class 내 상관관계로 해석할 수 있다. (총 분산으로 나누는 것은 이를 상관계수 지표로 바꿀 수 있다.)

더 일반적으로, 또 우리 추정 분산의 이전 개념을 참고하여, (한 클러스터에 대한) 공분산 행렬을 다음과 같이 볼 수 있다.

$$\Sigma = 
\left[
\begin{array}{ccc} 
\color{orange}{\sigma^2 + \tau^2} & \tau^2   & \tau^2  & \tau^2 & \tau^2 & \tau^2   \\
\tau^2   & \color{orange}{\sigma^2 + \tau^2} & \tau^2 & \tau^2 & \tau^2 & \tau^2    \\
\tau^2   & \tau^2   & \color{orange}{\sigma^2 + \tau^2} & \tau^2 & \tau^2 & \tau^2  \\
\tau^2   & \tau^2   & \tau^2 & \color{orange}{\sigma^2 + \tau^2} & \tau^2 & \tau^2\\
\tau^2   & \tau^2   & \tau^2  & \tau^2 & \color{orange}{\sigma^2 + \tau^2}  & \tau^2 \\
\tau^2   & \tau^2   & \tau^2  & \tau^2   & \tau^2  & \color{orange}{\sigma^2 + \tau^2} \\
\end{array}\right]$$
<br>

<span class="emph">compound symmetry</span> 공분산 구조를 나타낸다. 대부분의 mixed model 세팅에서 기본값인데, 앞에서 시각적으로 본 것과 같은 것이다. 이제 다른 종류의 공분산 구조들도 생각해보자.

간단하게 살펴보기 위해 개인에 대한 다음의 모형과 세개의 시점을 살펴보자.

$$\boldsymbol{y} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$$

정규 분포를 따르는 $y$ 세 관측값이 있다. 평균 $\mu$ 는 표준 회귀에서처럼 공변량들의 함수이다.

$$\mu = b_0 + b_1\cdot \mathrm{time} + b_2\cdot x_1 ...$$
마지막에 있는 $\epsilon$ 을 플롯하는 대신, 세 점 모두에 대해 전체 잔차 분산/공분산 구조를 정의하고 싶다.

표준 선형 회귀 모델 중 가장 간단한 세팅에서는 상수 분산과 공분산이 없다.

$$\Sigma = 
\left[
\begin{array}{ccc} 
\sigma^2 & 0   & 0   \\
0   & \sigma^2 & 0   \\
0   & 0   & \sigma^2 \\
\end{array}\right]$$

다음으로, 동일분산 가정을 풀고, 각 분산을 분리해서 추정할 수 있다. 이와 같은 이종분산 경우에 예를 들어 시간에 따라 분산이 오르락 내리락 할 수 있다.

$$\Sigma = 
\left[
\begin{array}{ccc} 
\sigma_1^2 & 0   & 0   \\
0   & \sigma_2^2 & 0   \\
0   & 0   & \sigma_3^2 \\
\end{array}\right]$$


우리가 사실 공분산/correlation 을 추정하고 싶다고 해보자. correlation 표현으로 바꿀 것이지만, 이 경우에도 분산이 상수거나 따로 추정된다고 가정해도 된다. 다음과 같은 것을 얻는데, $\rho$ 는 관측값 사이의 잔차 correlation 을 나타낸다.

$$\Sigma = \sigma^2
\left[
\begin{array}{ccc} 
1 & \rho_1   & \rho_2   \\
\rho_1   & 1 & \rho_3   \\
\rho_2   & \rho_3   & 1 \\
\end{array}\right]$$


이 경우 모든 시간 포인트 쌍에 대해 (상수 분산과) 다른 correlation 을 추정하게 된다. 이를 <span class="emph">unstructured</span>, 혹은 단순히 'symmetric' correlation 구조라고 부른다.

[mixed model 의 특수형태](https://m-clark.github.io/docs/mixedModels/anovamixed.html)인 반복수행 ANOVA를 알고 있다면, 흔한 가정은 <span class="emph">sphericity</span>, 혹은 <span class="emph">compound symmetry</span>의 느슨한 형태, 즉 모든 correlation 이 같은 값을 가지고 ($\rho_1=\rho_2=\rho_3$), 모든 분산이 같다는 것이 생각날 것이다. 

자주 사용되는 다른 correlation 구조(시간 기준 세팅에서)는 lag order 1 인 <span class="emph">autocorrelation</span> 잔차 구조이다.  What this means is that we assume the residuals at one time point apart correlate with some value $\rho$, observations at two time points apart correlate $\rho^2$, and so on.  As such we only need to estimate $\rho$, while the rest are then automatically determined.  Here's what it'd look like for four time points.

$$\Sigma = \sigma^2
\left[
\begin{array}{cccc} 
1 & \rho     & \rho^2   & \rho^3   \\
\rho     & 1 & \rho     & \rho^2   \\
\rho^2   & \rho     & 1 & \rho     \\
\rho^3   & \rho^2   & \rho     & 1 \\
\end{array}\right]$$


If $\rho$ was estimated to be .5, it would look like the following.

$$\Sigma = \sigma^2
\left[
\begin{array}{cccc} 
1 & .5       & .25      & .06   \\
.5       & 1 & .5       & .25  \\
.25      & .5       & 1 & .5    \\
.06      & .25      &  .5      & 1 \\
\end{array}\right]$$

Again, the main point is that points further apart in time are assumed to have less correlation. 

Know that there are many patterns and possibilities to potentially consider, and that they are not limited to the repeated measures scenario.  For example, the correlation could represent spatial structure, where units closer together geographically would be more correlated.  And as noted, we could also have variances that are different at each time point[^residstruct].  We'll start with that for the next example.

### 이종분산 

다른 많은 mixed model 패키지들은 잔차 공분산 구조를 모델링해 주지만[^lmerho], <span class="pack">lme4</span>는 적어도 [not in a straightforward fashion](https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html)으로 해주지는 않는다. 사실

does not provide the ability to model the residual covariance structure, at least [not in a straightforward fashion](https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html), though many other mixed model packages do[^lmerho].  In fact, two packages that come with the basic R installation do so, <span class="pack">mgcv</span> and <span class="pack">nlme</span>.  We'll demonstrate with the latter. 

The <span class="pack">nlme</span> package will have a different random effect specification, though not *too* different.  In addition, to estimate heterogeneous variances, we'll need to use an additional `weights` argument.  The following will allow each time point of occasion to have a unique estimate.


```{r heterovar, echo=1:5, eval=-5}
library(nlme)

heterovar_res = lme(
  gpa ~ occasion,
  data = gpa,
  random = ~ 1 | student,
  weights = varIdent(form = ~ 1 | occasion)
)

summary(heterovar_res)

extract_fixed_effects(heterovar_res) %>% 
  kable_df()

vc = extract_vc(heterovar_res, ci_level = 0)
vc %>% 
  kable_df()
```


At this point we're getting the same stuff we're used to. Now the not-so fun part.  For the values we're interested in for this example, i.e. the variances at each occasion, <span class="pack">nlme</span> does not make it easy on someone to understand initially, as the output regards the way things are for estimation, not for what one would usually have to report.  The variances are scaled relative to the first variance estimate, which is actually the reported residual variance in the random effects part. Additionally the values are also on the standard deviation rather than variance scale.  From the default output display, we can see that variance decreases over time in this case, but the actual values are not provided.

```{r heterovar_variances}
summary(heterovar_res$modelStruct)
```

Relative values are fine, I guess, but what we'd want are the actual estimates.  Here's how you can get them using the residual standard deviation to scale those values, then square them to get on the variance scale.

```{r heterovar_extract_vars_from_nlmes_cold_dead_hands}
(c(1.0000000, coef(heterovar_res$modelStruct$varStruct, unconstrained=F))*heterovar_res$sigma)^2
```

Yeah.  You'll have to look this up every time you want to do it, or just make your own function that takes the model input.  Here is how my function would extract the information.

```{r mixedup-extract_het_var}
mixedup::extract_het_var(heterovar_res, scale = 'var')
```

A newer alternative to keep in mind is <span class="pack">glmmTMB</span>.  It would allow one to stay more in the <span class="pack">lme4</span> style and output.

```{r glmmTMB_hetero}
library(glmmTMB)

heterovar_res2 = glmmTMB(
  gpa ~ occasion + (1|student) + diag(0 + occas |student), 
  data = gpa
)

summary(heterovar_res2)
```

Note that the variances displayed for each time point are not conflated with the residual variance. To compare with <span class="pack">nlme</span>, just add the residual variance to those estimates.  As with every mixed model package (apparently), it still takes a bit to get the variances in a usable form from the <span class="func">VarCorr</span> object. The following shows how though, and compares to the <span class="pack">nlme</span> result.

```{r extract-het-var-glmmtmb, eval = FALSE}
vc_glmmtmb = VarCorr(heterovar_res2)
vc_glmmtmb = attr(vc_glmmtmb$cond$student.1, 'stddev')^2 + sigma(heterovar_res2)^2
```

Here is the output from my function:

```{r mixedup-extract_het_var-tmb}
mixedup::extract_het_var(heterovar_res2, scale = 'var')
```

In any case, putting these together shows we get the same result.

```{r glmmTMB_hetero_compare_nlme, echo=F}
vc_glmmtmb = extract_het_var(heterovar_res2, scale = 'var')[-1] # remove group id
vc_nlme = extract_het_var(heterovar_res, scale = 'var')

names(vc_glmmtmb) = names(vc_nlme) = 
  unite(expand_grid(paste('year', 1:3), paste('sem', 1:2)), 'x', everything(), sep = ' ')$x

vc_compare = rbind(glmmTMB = vc_glmmtmb, nlme = vc_nlme) 

vc_compare %>% 
  kable_df()
```




### Autocorrelation

The following example shows the same basic model, but with the autocorrelation structure we described previously. In <span class="pack">nlme</span> we use the built-in <span class="func">corAR1</span> function and `correlation` argument similar to how we did with the weights argument.  

```{r corr_residual, echo=1:5, eval=-5}
library(nlme)

corr_res = lme(
  gpa ~ occasion,
  data = gpa,
  random = ~ 1 | student,
  correlation = corAR1(form = ~ occasion)
)

summary(corr_res)

extract_fixed_effects(corr_res) %>% 
  kable_df()

vc = extract_vc(corr_res, ci_level = 0)
vc %>% 
  kable_df()
```
<br>

Notice first that the fixed effect for occasion is the same as [before][Mixed model]. The variance estimates have changed slightly along with the variances of the fixed effects (i.e. the standard errors). The main thing is that we have a new parameter called `Phi` in the <span class="pack">nlme</span> output that represents our autocorrelation, `Phi`, with value of `r round(coef(corr_res$model$corStruct, unconstrained = F), 3)`. This suggests at least some correlation exists among the residuals for observations next to each other in time, though it diminishes quickly as observations grow further apart.

Note that glmmTMB will analyze such structure as well. Note that we need the factor form for occasion for this specification, and also note how it is part of model formula, like another random effect. More on this in the [supplemental section][Correlation Structure Revisited].

```{r corr_residual_glmmTMB, eval=FALSE, echo=1:4}
corr_res_tmb = glmmTMB(
  gpa ~ occasion +  ar1(0 + occas | student) + (1 | student),
  data = gpa
)

corr_res_brm = brms::brm(
  gpa ~ occasion +  ar(occasion, student) + (1 | student),
  data = gpa,
  cores = 4
)

summarise_model(corr_res)
summary(corr_res_tmb)
summarise_model(corr_res_brm)

extract_cor_structure(corr_res_tmb, which_cor = 'ar1')
extract_cor_structure(corr_res_brm)
extract_cor_structure(corr_res)
```



```{r glmmTMB_autocor, eval=FALSE, echo=FALSE}
# currently as of 2018-2 there is a bug in VarCorr at the end where it calls
# structure() when adding random intercept. Thus the model runs but one can't do
# anything with it because practically everything calls VarCorr; Inspection
# suggests the estimate (0.80877943) duplicates
# https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html and brms
# gets .83

# update 2020-09  brms and nlme are notably closer with gpa data using brms new approach to ar structure, while glmmTMB estimates no student variance and correlation value almost double the others with the gpa data

simCor1 <- function(
  phi = 0.8,
  sdgrp = 2,
  sdres = 1,
  npergrp = 20,
  ngrp = 20,
  seed = NULL,
  ## set linkinv/simfun for GLMM sims
  linkinv = identity,
  simfun = identity
) {
  
  if (!is.null(seed))
    set.seed(seed)
  cmat <- sdres * phi ^ abs(outer(0:(npergrp - 1), 0:(npergrp - 1), "-"))
  errs <- MASS::mvrnorm(ngrp, mu = rep(0, npergrp), Sigma = cmat)
  ranef <- rnorm(ngrp, mean = 0, sd = sdgrp)
  d <- data.frame(f = rep(1:ngrp, each = npergrp))
  eta <-
    ranef[as.numeric(d$f)] + c(t(errs)) ## unpack errors by row
  mu <- linkinv(eta)
  d$y <- simfun(mu)
  d$tt <- factor(rep(1:npergrp, ngrp))
  return(d)
}

d <- simCor1(
  phi = 0.8,
  sdgrp = 2,
  sdres = 1,
  seed = 101
)

corr_res2 = glmmTMB(gpa ~ occasion + (1 | student) + ar1(0 + occasion | student),
                    data = gpa)
summary(corr_res2)
VarCorr(corr_res2, sigma = .16707)
corr_res2$obj$env$report(corr_res2$fit$parfull)$corr[[2]]

test = brm(
  gpa ~ occasion + (1 | student),
  data = gpa,
  autocor = cor_ar(form = ~ occas),
  cores = 4
)

(lme_simple_fit <- lme(
  y ~ 1,
  random =  ~ 1 | f,
  data = d,
  correlation = corAR1()
))
glmmTMB_simple_fit <-
  glmmTMB(y ~ 1 + (1 | f) + ar1(tt - 1 | f), data = d, family = gaussian)
glmmTMB_simple_fit

glmmTMB_simple_fit$obj$env$report(glmmTMB_simple_fit$fit$parfull)$corr[[2]][2, 1]
corr_res2$obj$env$report(corr_res2$fit$parfull)$corr[[2]][2, 1]
```



## Generalized Linear Mixed Models

Just as generalized linear models extend the standard linear model, we can generalize (linear) mixed models to <span class="emph">generalized linear mixed models</span>.  Furthermore, there is nothing restricting us to only the exponential family, as other packages would potentially allow for many other response distributions.

For this example we'll do a logistic regression in the mixed model setting. In this case, we'll use the speed dating data set. In the speed dating events, the experiment randomly assigned each participant to ten short dates (four minutes) with other participants. For each date, each person rated six attributes (attractive, sincere, intelligent, fun, ambitious, shared interests) of the other person on a 10-point scale and wrote down whether he or she would like to see the other person again.

Our target variable is whether the participant would be willing to date the person again (`decision`).  To keep things simple, the predictors will be limited to the sex of the participant (`sex`), whether the partner was of the same race (`samerace`), and three of the attribute ratings the participant gave of their partner- attractiveness (`attractive`), sincerity (`sincere`), and intelligence (`intelligent`). The latter have been scaled to have zero mean and standard deviation of .5, which puts them on a more even footing with the binary covariates (`_sc`)[^scalecontbin].

```{r speed_dating, echo=FALSE, eval=FALSE}
speed_dating0 = readr::read_csv('data/raw_data/ARM_Data/Speed Dating Data.csv')

speed_dating = speed_dating0 %>%
  select(1:17, attr, sinc, intel, fun, amb, shar, dec) %>%
  rename(
    id_win_wave = id,
    sex = gender,
    partner_id = pid,
    n_met_in_wave = round,
    partner_age = age_o,
    partner_race = race_o,
    attractive = attr,
    sincere = sinc,
    intelligent = intel,
    fun = fun,
    ambitious = amb,
    shared_interests = shar,
    decision = dec
  ) %>%
  mutate(
    decision = factor(decision, labels = c('No', 'Yes')),
    sex = factor(sex, labels = c('Female', 'Male')),
    samerace = factor(samerace, labels = c('No', 'Yes')),
    attractive_sc = scale(attractive, scale = .5)[, 1],
    sincere_sc = scale(sincere, scale = .5)[, 1],
    intelligent_sc = scale(intelligent, scale = .5)[, 1],
    fun_sc = scale(fun, scale = .5)[, 1],
    ambitious_sc = scale(ambitious, scale = .5)[, 1],
    shared_interests_sc = scale(shared_interests, scale = .5)[, 1]
  ) %>%
  group_by(iid) %>%
  mutate(never_always = if_else(all(decision == 'Yes') |
                                  all(decision == 'No'), 1, 0)) %>%
  ungroup() %>%
  filter(never_always == 0) %>%  # as in Fahrmeier
  select(-never_always)
# describeAll(speed_dating)

save(speed_dating, file = 'data/speed_dating.RData')
```

```{r glmm_init, eval=FALSE, echo=FALSE}
# pretty much dupes fahrmeier although their table has a typo, and their would be 500, not 390 individuals after getting rid of constant
# sd_model = glmer(decision ~ sex*attractive_sc + sex*shared_interests_sc
#                  + (1|iid), data=speed_dating, family=binomial)   
load('data/speed_dating.RData')
sd_model = glmer(
  decision ~ sex + samerace + attractive_sc + sincere_sc
  + intelligent_sc
  + (1 | iid),
  data = speed_dating,
  family = binomial
)
summary(sd_model, correlation = F)
glmm_var_cor = tidy(VarCorr(sd_model)) %>%   # because for some reason knitr can't find an object it just used in the previous chunk.
  select(-var2) %>%
  rename(variance = vcov, sd = sdcor) %>%
  mutate_if(is.numeric, arm::fround, digits = 2)
save(sd_model, glmm_var_cor, file = 'data/speed_dating_model.RData')
```

```{r glmm_speed_dating, eval=FALSE}
load('data/speed_dating.RData')

sd_model = glmer(
  decision ~ sex + samerace + attractive_sc + sincere_sc + intelligent_sc
  + (1 | iid),
  data   = speed_dating,
  family = binomial
)

summary(sd_model, correlation = FALSE)
```


```{r glmm_fixed, echo=FALSE}
load('data/speed_dating_model.RData')

extract_fixed_effects(sd_model) %>% 
  kable_df()
```
<br>

The fixed effects results are as expected for the attributes, with attractiveness being a very strong effect in particular.  In addition, having a partner of the same race had a positive effect, while sex of the participant was statistically negligible.  You are free to exponentiate the coefficients to get the odds ratios if desired, just as you would with standard logistic regression.

<br>

```{r glmm_random, echo=FALSE, eval=TRUE}
extract_vc(sd_model, ci_level = 0) %>% 
  select(-var_prop) %>% 
  kable_df()
```

<br>

For the variance components, notice that there is no residual variance. This is because we are not modeling with the normal distribution for the response, thus there is no $\sigma$ to estimate.  However, the result suggests that there is quite a bit of variability from person to person.


## Exercises for Extensions


### Sociometric data

In the following data, kids are put into different groups and rate each other in terms of how much they would like to share some activity with the others. We have identifying variables for the person doing the rating (sender), the person being rated (receiver), what group they are in, as well as age and sex for both sender and receiver, as well as group size.


```{r socio_setup, echo=FALSE, eval=FALSE}
soc = read_spss('data/raw_data/joop_hox_data2/9 CrossClass/SocsLong.sav')
glimpse(soc)


sociometric = soc %>% 
  mutate(sexsend = factor(sexsend, labels = c('Male', 'Female')),  # from text 0 male, 1 female
         sexrec = factor(sexrec, labels = c('Male', 'Female')))

save(sociometric, file='data/sociometric.RData')
```

To run a mixed model, we will have three sources of structure to consider:

- senders (within group)
- receivers (within group)
- group

First, load the sociometric data. 

```{r load_socio}
load('data/sociometric.RData')
```


To run the model, we will proceed with the following modeling steps. For each, make sure you are creating a separate model object for each model run.

- Model 1: No covariates, only sender and receiver random effects. Note that even though we don't add group yet, still use the nesting approach to specify the effects (e.g. `1|group:receiver`)
- Model 2: No covariates, add group random effect
- Model 3: Add all covariates: `agesend/rec`, `sexsend/rec`, and `grsize` (group size)
- Model 4: In order to examine sex matching effects, add an interaction of the sex variables to the model `sexsend:sexrec`.
- Compare models with AIC (see the note about [model comparison][model comparison]), e.g. `AIC(model1)`. A lower value would indicate the model is preferred.


```{r socio, echo=F, eval=FALSE}
model1 = lmer(rating ~ (1|group:sender) + (1|group:receiver), 
              data = sociometric)
summary(model1, correlation = FALSE) 

model2 = lmer(rating ~ (1|group:sender) + (1|group:receiver) + (1|group), 
              data = sociometric)
summary(model2, correlation = F) 

model3 = lmer(rating ~ sexsend + sexrec + agesend + agerec + grsize + (1|group:sender) + (1|group:receiver) + (1|group), 
             data = sociometric)
summary(model3, correlation = FALSE)

model4 = lmer(
  rating ~ sexsend*sexrec + agesend + agerec + grsize +
    (1|group:sender) + (1|group:receiver) + (1|group), 
  data = sociometric)
summary(model4, correlation = FALSE)

c(AIC(model1), AIC(model2), AIC(model3), AIC(model4))
```




### Patents

Do a Poisson mixed effect model using the [patent data][Data].  Model the number of citations (`ncit`) based on whether there was opposition (`opposition`) and if it was for the biotechnology/pharmaceutical industry (`biopharm`). Use year as a random effect to account for unspecified economic conditions.  

```{r patent_setup, echo=FALSE, eval=FALSE}
patents0 = readr::read_tsv('data/raw_data/patent.raw')
patents = patents0 %>% 
  rename(opposition = opp) 
save(patents, file='data/patents.RData')
glmer(ncit ~ opposition +  biopharm + (1|year), data=patents, family='poisson')
```


```{r patent_starter, eval=FALSE}
load('data/patents.RData')
```


Interestingly, one can model overdispersion in a Poisson model by specifying an random intercept for each observation (`subject` in the data).  In other words, no specific clustering or grouped structure is necessary, but we can use the random effect approach to get at the extra variance.




[^residstruct]: One reason to do so would be that you expect variability to decrease over time, e.g. due to experience.  You might also allow that variance to be different due to some other grouping factor entirely (e.g. due to treatment group membership).

[^lmerho]: This feature request has been made by its users for over a decade at this point- it's not gonna happen.  The issue is that the way <span class="pack">lmer</span> works by default employs a method that won't allow it (this is why it is faster and better performing than other packages). Unfortunately the common response to this issue is 'use <span class="pack">nlme</span>'.  However many other packages work with <span class="pack">lme4</span> rather than <span class="pack">nlme</span>, and if you aren't going to use <span class="pack">lme4</span> for mixed models you might as well go Bayesian with <span class="pack">rstanarm</span> or <span class="pack">brms</span> instead of <span class="pack">nlme</span>.  I would even prefer <span class="pack">mgcv</span> to <span class="pack">nlme</span> (though it can use <span class="pack">nlme</span> under the hood) because of the other capabilities it provides, and the objects created are easier to work with in my opinion.

[^crossed_notation]: I don't show the formal model here as we did before, but this is why depicting mixed models solely as 'multilevel' becomes a bit problematic in my opinion. In the standard mixed model notation it's straightforward though, you just add an additional random effect term, just as we do in the actual model syntax.

[^signflip]: Setting aside our discussion to take a turn regarding regression modeling more generally, this is a good example of 'surprising' effects not being so surprising when you consider them more closely.  Take a look at the effect of experience. More experience means less stress, this is probably not surprising.  Now look at the age effect. It's positive! But wouldn't older nurses have more experience? What's going on here? When interpreting experience, it is with age *held constant*, thus more experience helps with lowering stress no matter what your age.  With age, we're holding experience constant.  If experience doesn't matter, being older is affiliated with more stress, which might be expected given the type of very busy and high pressure work often being done (the mean age is `r median(nurses$age)`).  A good way to better understand this specifically is to look at predicted values when age is young, middle, and older vs. experience levels at low, middle, and high experience, possibly explicitly including the interaction of the two in the model.  Also note that if you take experience out of the model, the age effect is negative, which is expected, as it captures experience also.

[^crossnest]: Just a reminder, it *does* matter if you label your data in a less than optimal fashion.  For example, if in the nesting situation you start your id variable at 1 for each nested group, then you have to use the nested notation in <span class="pack">lme4</span>, otherwise, e.g. it won't know that id = 1 in group 1 is different from id 1 in group 2.  In our hospital example, this would be akin to using `ward` instead of `wardid` as we did.  Again though, this wouldn't be an issue if one practices good data habits.  Note also the `:` syntax. In other modeling contexts in R this denotes an interaction, and that is no different here.  In some contexts, typically due to experimental designs, one would want to explore random effects of the sort 1|A, 1|B and 1|A:B.  However, this is relatively rare.

[^scalecontbin]:  Note that for a balanced binary variable, the mean `p=.5` and standard deviation is `sqrt(p*(1-p)) = .5`